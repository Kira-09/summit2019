---
title: "Introduction to dplyr & R notebooks"
author: "Trevor Paulsen"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.


# Reading a datafeed file
To start, let's read in a sample datafeed file:

```{r}
setwd("/Users/tpaulsen/Desktop/Product Management/Summit/2019/R/")
data_feed = read.table(file = "example_datafeed.tab", sep="\t", header = TRUE, na.strings = "", stringsAsFactors = FALSE)
head(data_feed)
```


# Reading a classification file
Read in a sample classification file:

```{r}
classification = read.table(file="example_classification.tab", sep="\t", header = TRUE, stringsAsFactors = FALSE)
head(classification)
```


# Dplyr basics
Now let's talk about dplyr!
Dplyr is a very popular R library that boils down complex data manipulation tasks into a series of verbs. It's kinda similar to SQL, but way easier to use and you can do a lot more with much less code.

Go here for more examples and information: https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html

```{r}
# Install dplyr using install.packages("dplyr")
library(dplyr)

# FILTER: filter the classification to just the Email campaigns
email_campaigns = classification %>%
  filter(channel == "Email")

# ARRANGE: sort the datafeed by hit time gmt
sorted_data_feed = data_feed %>%
  arrange(hit_time_gmt)

# SELECT: select only certain columns of a datafeed (like a SQL select) and rename them
couple_columns_from_data_feed = data_feed %>%
  select(
    uuid = user_id,
    timestamp = hit_time_gmt
  )

# MUTATE: create a new column using some calculation
data_feed_with_modifications = data_feed %>%
  mutate(
    # concatenate visitor id hi and low
    visitor_id = paste0(visitor_id_hi, "_", visitor_id_lo)
  )

# GROUP BY + SUMMARISE: perform operations on groupings of data
visitor_level_aggregations = data_feed_with_modifications %>%
  group_by(visitor_id) %>%
  summarise(
    rows = n(),
    visits = n_distinct(visit_num)
  )
head(visitor_level_aggregations)
```


# Connecting dplyr to an external database
The best part about dplyr is that you can use it to interface with nearly any system that accepts SQL commands, not just on your local machine!

This means using the same dplyr commands above, you can run the same operations at scale on databases like MySQL, MariaDB, Postgres interfaces (including Amazon Redshift), SQLite, odbc, or Google BigQuery. You can also interface with Adobe's Cloud Platform this way!

To make it work, just install "dbplyr" (install.packages("dbplyr") to install) in addition to dplyr above. See this link for more information: https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html

```{r}

# First, create a connection to your database of choice. Each database will have its own parameters to enter, but the first parameter is always the type of database

# Typical example:
con = DBI::dbConnect(RSQLite::SQLite(), 
  host = "database.host.com",
  user = "myuser",
  
  # Putting your password in a string isn't safe! Rstudio gives this handy option:
  password = rstudioapi::askForPassword("Database password")
)

# Here's a bunch of options you can put as the first parameter:
RSQLite::SQLite()
RMySQL::MySQL()
RPostgreSQL::PostgreSQL()
odbc::odbc()
bigrquery::bigquery()
```


If you use an on premise setup, you can connect dplyr directly to a SparkSQL backend using a library called "sparklyr" which is also amazing. You can read all about that here: https://spark.rstudio.com/

For the rest of this session, we'll just use a local file so that results can be easily reproduced, but I'll add comments to show you how you'd do things if connected to an external database.






